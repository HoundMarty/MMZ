
## Optimální filtry ve smyslu kvadratické vzdálenosti: Wienerův filtr

### Teoretické

1. Least Mean Square (LMS) nebo též Wienerův filtr: princip a odvození.

----

## **Minimalizace kvadratické vzdálenosti**

$x[n]$ je vstupní, reálný signál.

Signál zpracováváme FIR filtrem *w* o délce *L*, výstupem je 

$
\begin{equation}
y[n] = w[0]x[n]+w[1]x[n-1]+\dots+w[L-1]x[n-L] = \sum_{k=0}^{L-1}w[k]x[n-k]
\end{equation}
$

což lze zapsat vektorově jako

$
\begin{equation}
y[n] = \textbf{w}^T\textbf{x}_n
\end{equation}
$

kde

$
\begin{equation}
\textbf{x}_n = \begin{bmatrix}
x[n] \\
x[n-1] \\
\vdots \\
x[n-L]
\end{bmatrix}
\textbf{w} = \begin{bmatrix}
w[0] \\
w[1] \\
\vdots \\
w[L]
\end{bmatrix}
\end{equation}
$

$d[n]$ je signál, který chceme získat filtrováním signálu $x[n]$.

Chyba rozdílu je popsána následovně

$
\begin{equation}
    \begin{split}
        e[n] &= d[n] - y [n]  \\
        &= d[n] - \textbf{w}^T\textbf{x}_n
    \end{split}
\end{equation}
$

Filtr *w* můžeme hledat jako optimalizaci nějakého kritéria, např. kvadratického

$
\begin{equation}
    \begin{split}
        J_n[\textbf{w}] &= e[n]^2  \\
        &= (d[n] - \textbf{w}^T\textbf{x}_n)^2 \\
        &= d[n]^2 - 2d[n]\textbf{w}^T\textbf{x}_n + \textbf{w}^T\textbf{x}_n \textbf{x}_n^T\textbf{w}
    \end{split}
\end{equation}
$

## **Gradient $\textit{J}_n$($\textbf{w}$)**

Gradient $\textit{J}_n$($\textbf{w}$), tj. vektor parciálních derivací podle jednotlivých složek $\textbf{w}$

$
\begin{equation}
    \Delta \textit{J}_n(\textbf{w}) = -2\textbf{x}_nd(n) + 2\textbf{x}_n \textbf{x}_n^T \textbf{w}
\end{equation}
$

zavádíme označení

$
\begin{equation}
    \begin{split}
        \textbf{R}_n &= \textbf{x}_n \textbf{x}_n^T \\
        \textbf{p}_n &= \textbf{x}_n d[n]
    \end{split}
\end{equation}
$

Gradient lze zapsat také jako

$
\begin{equation}
    \Delta \textit{J}_n(\textbf{w}) = -2\textbf{p}_n + 2 \textbf{R}_n \textbf{w}
\end{equation}
$

## **Least Mean Square**

Položíme-li $\Delta\textit{J}_n(\textbf{w})$ roven nule, dostáváme rovnici

$
\begin{equation}
    \textbf{R}_n\textbf{w} = \textbf{p}_n
\end{equation}
$

Matice $\textbf{R}_n$ má hodnost 1 $\longrightarrow$ takže nemá inverzi.

Definujeme nové kritérium zprůměrováním $\textit{J}_n(\textbf{w})$ přes interval $\textit{n} = 1, \dots, N$

$
\begin{equation}
    \begin{split}
        \textit{J}_{LMS}(\textbf{w}) &= \frac{1}{N}\sum_{n=1}^{N} \textit{J}_{n}(\textbf{w}) \\
        &=  \frac{1}{N}\sum_{n=1}^{N} e[n]^2 
    \end{split}
\end{equation}
$

Gradient $\textit{J}_{LMS}(\textbf{w})$ je průměr gradientů $\textit{J}_{n}(\textbf{w})$

$
\begin{equation}
    \Delta \textit{J}_{LMS}(\textbf{w}) = \frac{1}{N}\sum_{n=1}^{N} \Delta \textit{J}_{n}(\textbf{w})
\end{equation}
$

Takže

$
\begin{equation}
    \Delta \textit{J}_{LMS}(\textbf{w}) = -2 \textbf{p} + 2 \textbf{Rw}
\end{equation}
$

kde

$
\begin{equation}
    \begin{split}
        \textbf{R} &= \frac{1}{N}\sum_{n=1}^{N} \textbf{x}_n \textbf{x}_n^T = \frac{1}{N}\sum_{n=1}^{N}\textbf{R}_n \\
        \textbf{p} &= \frac{1}{N}\sum_{n=1}^{N} \textbf{x}_n d[n] = \frac{1}{N}\sum_{n=1}^{N}\textbf{p}_n 
    \end{split}
\end{equation}
$

Matice $\textbf{R}$ již může mít inverzi (je-li interval $\textit{n} = 1,\dots,N$ dostatečně dlouhý). Proto

$
\begin{equation}
    \textbf{w}_{LMS} = \textbf{R}^{-1}\textbf{p}
\end{equation}
$

**Prvky $\textbf{R}$ jsou hodnoty auto-kovariance x**

$
\begin{equation}
    \begin{split}
        \textbf{R}_{ij} &= \frac{1}{N}\sum_{n=1}^{N} x[n-i+1]x[n-j+1] \\
        &= \hat{C}_{xx}[i-j]
    \end{split}
\end{equation}
$

Platí tedy (až na koncové prvky)

$
\begin{equation}
    \textbf{R} = \begin{pmatrix}
        \hat{C}_{xx}[0] & \hat{C}_{xx}[1] & \hat{C}_{xx}[2] & \cdots & \hat{C}_{xx}[L-1] \\
        \hat{C}_{xx}[1] & \hat{C}_{xx}[0] & \hat{C}_{xx}[1] & \cdots & \hat{C}_{xx}[L-2] \\
        \hat{C}_{xx}[2] & \hat{C}_{xx}[1] & \hat{C}_{xx}[0] & \cdots & \hat{C}_{xx}[L-3] \\
        \vdots & \vdots & \vdots & \ddots  & \vdots \\
        \hat{C}_{xx}[L-1] & \hat{C}_{xx}[L-2] & \hat{C}_{xx}[L-3] & \cdots & \hat{C}_{xx}[0]
        \end{pmatrix}
\end{equation}
$

Matici $\textbf{R}$ lze zapsat též jako 

$
\begin{equation}
    \textbf{R} = \frac{1}{N} \textbf{XX}^T
\end{equation}
$

kde

$
\begin{equation}
    \textbf{X} = \begin{pmatrix}
        x[1] & x[2] & \cdots & \cdots & x[N] & 0 & 0 & \cdots & 0\\
        0 & x[1] & \cdots & \cdots & x[N-1] & x[N] & 0 & \cdots & 0\\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & \cdots & x[1] & \cdots & x[N-L] & \cdots & \cdots & \cdots & x[N]\\
        \end{pmatrix}
\end{equation}
$

$\textbf{R}$ je Toeplitzovská (diagonálně konstantní), symetrická a positivně semidefinitní (má reálná nezáporná vlastní čísla).

**Prvky $\textbf{p}$ jsou hodnoty Cross-kovariance**

Cross-kovariance

$
\begin{equation}
    \textbf{p}_j = \frac{1}{N}\sum_{n=1}^{N} x[n-j+1] d[n]
\end{equation}
$

lze zapsat také jako

$
\begin{equation}
    \textbf{p}_j = \frac{1}{N}\textbf{Xd}
\end{equation}
$

kde 

$
\begin{equation}
    \textbf{d} = \left[ d[1],  d[2],  \cdots  d[N] \right]^T
\end{equation}
$

**Řešení soustavy $\textbf{Rw}_{LMS} = \textbf{p}$**

+ Řešení soustavy $\textbf{Rw}_{LMS} = \textbf{p}$ vyžaduje $\mathcal{O}(L^3)$ operací.
+ Levinson-Durbinův algoritmus využívá toeplitzovskou strukturu $\textbf{R}$ a počítá pomocí rekuzre řešení pro všechny délky filtru $1,\dots,L$ přičemž složitost je $\mathcal{O}(L^2)$.
+ Rovněž paměťové nároky jsou řádově nižší, protože neukládáme do paměti celou matici $\textbf{R}$, pouze její řádek (sloupec).

## **Wienerův filtr**

**Předpoklad** $x$ a $d$ jsou slabě stacionární (jejich auto-kovariační funkce (a tedy spektrum) se v čase nemění).

Kritérium definujeme jako 

$
\begin{equation}
    \textit{J}(\textbf{w}) = E[e[n]^2]
\end{equation}
$
Díky stacionaritě je toto kritérium nezávislé na $n$ (na čase)

Gradient $\textit{J}(\textbf{w}) položíme roven nule, takže

$
\begin{equation}
    \textbf{w}_{wiener} = \text{arg}\min_{\textbf{w}} \textit{J}(\textbf{w}) = \textbf{R}^{-1}\textbf{p}
\end{equation}
$
kde

$
\begin{equation}
    \begin{split}
        \textbf{R} &= E \left[ \textbf{x}_n \textbf{x}_n^T \right] \\
        \textbf{p} &= E \left[ \textbf{x}_n d[n] \right]
    \end{split}
\end{equation}
$


## **LMS vs. Wienerův filtr**

Porovnání definice $\textbf{R}$ a $\textbf{p}$ pro LMS a Wienerův filter.

$ 
\begin{equation}
    \begin{matrix}
        \text{Wienerův filter}  & \qquad &  \text{LMS}  \\
        \textbf{R} = E \left[ \textbf{x}_n \textbf{x}_n^T \right] & \qquad &  \textbf{R} = \frac{1}{N} \textbf{XX}^T \\
        & & \\
        \textbf{p} = E \left[ \textbf{x}_n d[n] \right] & \qquad & \textbf{p}_j = \frac{1}{N}\textbf{Xd}
    \end{matrix}
\end{equation}
$

Jsou-li $x$ a $d$ stacionární, potom

$
\begin{equation}
    \textbf{w}_{LMS} \overset{N\to + \infty}{\longrightarrow} \textbf{w}_{wiener}
\end{equation}
$

Pokud signály nejsou stacionární, nelze nic obecně usoudit.